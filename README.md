# Projeto Bikestore
---
## Objetivo
Esse reposit√≥rio foi criado para realiza√ß√£o de um treinamento b√°sico na plataforma do Databricks. Estamos usando a vers√£o Free Edition e GitHub como ambiente.

---
## Dados do projeto
Optamos por usar o conjunto de dados dispon√≠vel no Kaggle, [Bikestore](https://www.kaggle.com/datasets/dillonmyrick/bike-store-sample-database/data), por conter um volume de registros e arquivos bons para exemplificarmos as principais funcionalidades.

---
## Conceitos principais

**Nosso Objetivo:** Criar a estrutura de cat√°logos, schemas e volumes necess√°rios para o workshop

### O que √© a Arquitetura Medallion?

A arquitetura **Medallion** √© um padr√£o de design para organizar dados em lakehouse, dividindo em 3 camadas:

- **ü•â Bronze (Raw)**: Dados brutos, c√≥pia fiel da origem
  - Formato: como veio da fonte
  - Prop√≥sito: auditoria, reprocessamento, hist√≥rico

- **ü•à Silver (Refined)**: Dados limpos e conformados
  - Formato: validado, tipado, deduplicado
  - Prop√≥sito: dados confi√°veis para an√°lise

- **ü•á Gold (Curated)**: Dados prontos para consumo
  - Formato: agregado, otimizado, modelado
  - Prop√≥sito: dashboards, relat√≥rios, ML

_NOTA_: esse √© um padr√£o que a Databricks vem utilizando. Outros _vendors_ adotam nomes diferentes para essa arquitetura. Alguns exemplos:

- Camada Bronze - Nomes Comuns: `RAW`, `LANDING`, `STAGING`                     
- Camada Silver - Nomes Comuns: `STAGING` (para transforma√ß√£o), `REFINED`, `CLEANSED`,`CONFORMED`                                                                              
- Camada Gold - Nomes Comuns: `ANALYTICS`, `CONSUMPTION`, `PRESENTATION`, `MART`, `BUSINESS`

H√° quem opte por criar uma camada adicional (como n√≥s) para armazenar os dados que est√£o chegando, antes de carregar na bronze. Isso pode ser √∫til para fins de auditoria em ambientes que precisam de uma rastreabilidade maior.

---

## Unity Catalog: Cora√ß√£o da plataforma

No Databricks, os dados s√£o organizados em 3 n√≠veis:

```
Catalog (equivalente a "Database" em SQL tradicional)
  ‚îî‚îÄ‚îÄ Schema (equivalente a "Schema" em PostgreSQL/SQL Server)
       ‚îú‚îÄ‚îÄ Tables (tabelas gerenciadas ou externas)
       ‚îú‚îÄ‚îÄ Views (vis√µes SQL)
       ‚îî‚îÄ‚îÄ Volumes (armazenamento de arquivos n√£o estruturados)
```
Toda estrutura de Governan√ßa, acesso, auditabilidade, linhagem e afins se d√° atrav√©s do Unity Catalog.

Dentro da nossa estrutura de cat√°logo, podemos ter tabelas, views, fun√ß√µes e volumes.

**Volumes** s√£o diret√≥rios gerenciados pelo Unity Catalog para armazenar arquivos n√£o estruturados:
- CSVs, JSONs, Parquet, imagens, etc.
- Acesso via `dbfs:/Volumes/{catalog}/{schema}/{volume}/`
- Suportam controle de acesso (ACLs)
- Ideal para armazenar dados brutos antes da ingest√£o

**Para acessar - Comparativo:**
- SQL tradicional: `Database.Schema.Table`
- Databricks: `Catalog.Schema.Table`

_NOTA:_ Nesse treinamento n√£o abordamos quest√µes envolvendo volumes MANAGED x EXTERNAL. Por hora, e de forma simples, usamos volumes MANAGED por serem geridos em termos de otimiza√ß√£o, espa√ßo e performance pelo Databricks. Volumes EXTERNAL exigem uma gest√£o adicional.

---
## Al√©m desses, ainda temos:

‚úÖ **PySpark DataFrames**: Estrutura de dados distribu√≠da do Spark
- Diferente de Pandas DataFrame (distribu√≠do vs em mem√≥ria)
- Lazy evaluation: c√≥digo cria plano de execu√ß√£o, n√£o executa imediatamente
- Actions (write, show, count) disparam a execu√ß√£o

‚úÖ **Delta Lake**: Formato de armazenamento ACID
- Vantagens: transa√ß√µes ACID, time travel, MERGE/UPDATE/DELETE
- Melhor que CSV/Parquet para lakehouses
- Suporta schema evolution e otimiza√ß√µes avan√ßadas

‚úÖ **Tabelas Gerenciadas vs Externas**:
- **Gerenciada** (`.saveAsTable()`): Databricks controla dados + metadados
- **Externa** (`.save(path)`): Voc√™ controla localiza√ß√£o dos dados

‚úÖ **Opera√ß√µes PySpark fundamentais**:
- `spark.read.csv()` - Ler arquivos CSV
- `.option()` - Configurar par√¢metros de leitura
- `.withColumn()` - Adicionar/transformar colunas
- `.write.mode().format().saveAsTable()` - Gravar tabelas

‚úÖ **MERGE INTO**: Comando SQL ACID para upsert
- Combina INSERT + UPDATE em uma opera√ß√£o at√¥mica
- Mais eficiente que 3 opera√ß√µes separadas
- Exclusivo do Delta Lake (n√£o existe em CSV/Parquet)

‚úÖ **row_hash**: T√©cnica de detec√ß√£o de mudan√ßas
- SHA2 + CONCAT_WS para criar hash √∫nico dos dados
- Compara 1 string ao inv√©s de N colunas (performance!)
- Evita UPDATEs desnecess√°rios quando dados n√£o mudaram

‚úÖ **try_cast()**: Convers√£o segura de tipos
- Retorna NULL se convers√£o falhar (ao inv√©s de erro)
- Essencial quando tipos na Bronze s√£o inconsistentes
- Exemplo: `try_cast(date_string as DATE)`

‚úÖ **SCD Type 1**: Slowly Changing Dimensions
- Sobrescreve valores antigos (n√£o mant√©m hist√≥rico)
- Alternativas: SCD Type 2 (hist√≥rico completo), Type 3 (valor anterior)


‚úÖ **Orquestra√ß√£o Python + SQL**: Combinando o melhor dos dois mundos
- Python: l√≥gica condicional, loops, controle de fluxo
- SQL: manipula√ß√£o de dados declarativa
- `spark.sql()`: executa SQL a partir de Python

‚úÖ **Estrat√©gias de Carga**:
- **Full Load** (`INSERT OVERWRITE`): primeira execu√ß√£o, mais r√°pido
- **Incremental** (`MERGE INTO`): execu√ß√µes subsequentes, mais eficiente para updates

‚úÖ **WHEN NOT MATCHED BY SOURCE**: Conceito avan√ßado do MERGE
- Deleta registros da target que n√£o existem mais na source
- Tudo em uma transa√ß√£o ACID (atomicidade garantida)

‚úÖ **Decis√£o Condicional**:
```python
is_empty = spark.sql(f"SELECT COUNT(1) FROM {table}").first()[0] == 0
```
- `.first()`: pega primeira linha (Row object)
- `[0]`: acessa primeira coluna (COUNT)
- `== 0`: verifica se est√° vazio

‚úÖ **F-strings em Python**: Interpola√ß√£o de vari√°veis
```python
f"INSERT OVERWRITE {target_table} SELECT * FROM {source_query}"
```
- Substitui `{variavel}` pelo valor da vari√°vel
- Mais leg√≠vel que concatena√ß√£o de strings

‚úÖ **Wide Tables**: Desnormaliza√ß√£o para an√°lise
- JOIN entre m√∫ltiplas tabelas dimensionais
- Reduz necessidade de JOINs em queries de consumo
- Trade-off: duplica√ß√£o de dados vs performance de leitura

---

## ‚ö†Ô∏è Troubleshooting
**Erro: "Path does not exist: dbfs:/Volumes/..."**
- Verifique se os arquivos CSV est√£o no volume
- Confirme o caminho: `dbfs:/Volumes/c_bikeshop/raw/raw_files/brands.csv`
- Use o Data Explorer para navegar at√© o volume

**Erro: "Cannot parse date/timestamp"**
- Normal se CSV tem formatos de data inconsistentes
- `inferSchema` tenta adivinhar, mas pode falhar
- Solu√ß√£o: defina schema expl√≠cito ou use `try_cast` na Silver

**Performance lenta no inferSchema**
- `inferSchema=True` l√™ dados 2 vezes (primeira para inferir, segunda para carregar)
- Em produ√ß√£o: sempre defina schema expl√≠cito
- Exemplo: `.schema("brand_id INT, brand_name STRING")`

**Tabelas n√£o aparecem no Data Explorer**
- Aguarde alguns segundos e recarregue
- Verifique se voc√™ est√° olhando o cat√°logo/schema correto

**Erro: "Table or view not found: c_bikeshop.bronze.brands"**
- Certifique-se que o notebook 00_setup foi executado
- Verifique se a c√©lula de ingest√£o executou sem erros

**Erro: "Table or view not found: _nome_tabela_ "**
- Tabelas podem ter depend√™ncias! Execute c√©lulas em ordem
- `sales_by_day` e `customer_lifetime_value` dependem de `bike_sales`

**Erro: "Cannot resolve column 'product_id'"**
- Verifique se tabelas Silver foram criadas corretamente
- Execute: `SHOW TABLES IN silver` para confirmar

**Performance lenta na primeira execu√ß√£o**
- Normal! INSERT OVERWRITE processa todos os dados
- Execu√ß√µes subsequentes (MERGE) s√£o mais r√°pidas

**MERGE sempre faz full scan**
- MERGE precisa comparar source vs target (inevit√°vel)
- Para tabelas muito grandes, considere particionamento
- Exemplo: `PARTITIONED BY (order_date)`

**Erro: "Syntax error near 'BY SOURCE'"**
- `WHEN NOT MATCHED BY SOURCE` requer Delta Lake
- Verifique: `DESCRIBE EXTENDED gold.table` ‚Üí Provider = delta

**count() retorna erro ao tentar [0]**
- spark.sql() retorna DataFrame, n√£o valor direto
- Sempre use `.first()[0]` para acessar valor escalar

**row_hash sempre diferente (sempre faz UPDATE)**
- Certifique-se que campos no CONCAT_WS est√£o na mesma ordem
- Cuidado com NULL: `CONCAT_WS('|', NULL, 'value')` pode causar hashes diferentes
- Use COALESCE para tratar NULLs: `COALESCE(field, '')`

**Erro: "try_cast is not a valid function"**
- try_cast √© do Spark SQL 3.0+
- Alternativa: `CAST ... AS ... ` com tratamento de erro separado


---

## üîç Comandos √öteis para Explorar

```sql
-- Ver todas as camadas do lakehouse
SHOW SCHEMAS IN c_bikeshop;

-- Comparar contagens entre camadas
SELECT 'bronze' as layer, COUNT(*) as tables FROM (SHOW TABLES IN bronze);
-- (repita para silver e gold)

-- Analizar performance de queries
EXPLAIN SELECT * FROM gold.bike_sales WHERE order_date = '2018-01-01';

-- Ver hist√≥rico de uma tabela Gold
DESCRIBE HISTORY gold.bike_sales;

-- Ver tamanho das tabelas
DESCRIBE DETAIL gold.bike_sales;

-- Ver hist√≥rico de vers√µes (Time Travel)
DESCRIBE HISTORY c_bikeshop.bronze.brands;

-- Consultar vers√£o antiga
SELECT * FROM c_bikeshop.bronze.brands VERSION AS OF 1;
```
---
## Links √∫teis:
- Exemplos de c√≥digo Spark: https://sparkbyexamples.com/
- Doc. Databricks: https://docs.databricks.com/aws/en/getting-started/
- Doc. Databricks - PySpark: https://docs.databricks.com/aws/en/pyspark
- Padr√£o de commits: https://www.conventionalcommits.org/en/v1.0.0/#summary




